import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

deepseek_prompt = """
–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ç–µ—Å—Ç–æ–≤ –∏ –æ–ø—Ä–æ—Å–æ–≤.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.

‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: —Ç–µ—Å—Ç –∏ –æ–ø—Ä–æ—Å - —ç—Ç–æ –†–ê–ó–ù–´–ï –≤–µ—â–∏! –ù–µ –ø—É—Ç–∞–π –∏—Ö!
üìå –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã - —ç—Ç–æ –¢–ï–°–¢.
üìå –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ—Ç - —ç—Ç–æ –û–ü–†–û–°.

üîπ **–§–æ—Ä–º–∞—Ç –¥–ª—è –û–ü–†–û–°–ê:**
{
  "type": "survey",
  "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –æ–ø—Ä–æ—Å–∞",
  "questions": [
    {
      "text": "–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞",
      "options": ["–û—Ç–≤–µ—Ç 1", "–û—Ç–≤–µ—Ç 2", "–û—Ç–≤–µ—Ç 3"]
    }
  ]
}

üîπ **–§–æ—Ä–º–∞—Ç –¥–ª—è –¢–ï–°–¢–ê:**
{
  "type": "test",
  "title": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞",
  "questions": [
    {
      "text": "–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞",
      "options": [
        {"text": "–û—Ç–≤–µ—Ç 1", "correct": False},
        {"text": "–û—Ç–≤–µ—Ç 2", "correct": True}
      ]
    }
  ]
}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π! –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å, –≤–µ—Ä–Ω–∏ `{"error": "–û—à–∏–±–∫–∞"}`.

–¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:
{input_text}
"""

model_path = "F:/BOT_TELEGRAM/PollBot/deepseek_models"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

cached_model = None
cached_tokenizer = None
cached_generator = None


def load_deepseek_model():
    global cached_model, cached_tokenizer, cached_generator

    if cached_model is None:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º DeepSeek –≤ –ø–∞–º—è—Ç—å...")
        cached_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        ).to(device)
        cached_tokenizer = AutoTokenizer.from_pretrained(model_path)
        cached_generator = pipeline(
            "text-generation",
            model=cached_model,
            tokenizer=cached_tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å DeepSeek –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å!")

    return cached_generator


def process_text_with_deepseek(text):
    generator = load_deepseek_model()

    prompt = deepseek_prompt.format(input_text=text)

    result = generator(
        prompt,
        max_new_tokens=500,
        temperature=0.5,
        top_p=0.9,
        repetition_penalty=1.2,
        do_sample=True
    )[0]["generated_text"]

    print("üîç –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n", result)  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º JSON
    try:
        parsed_json = json.loads(result.strip())

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–ª—é—á "type"
        if "type" not in parsed_json:
            print("‚ùå –û—à–∏–±–∫–∞: JSON –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'type'.")
            return None

        return parsed_json
    except json.JSONDecodeError:
        print("‚ùå –û—à–∏–±–∫–∞: DeepSeek –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON.")
        return None  # –í–µ—Ä–Ω—ë–º None, —á—Ç–æ–±—ã –±–æ—Ç –Ω–µ —É–ø–∞–ª
