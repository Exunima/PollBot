import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
model_path = "F:/BOT_TELEGRAM/PollBot/Mistral-7B"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
).to(device)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
print("‚úÖ Mistral-7B –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")


def extract_json(text):
    """
    –ò—â–µ—Ç –≤—Å–µ JSON-–±–ª–æ–∫–∏ –ø–æ —Å–∫–æ–±–∫–∞–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞–ª–∏–¥–Ω—ã–π –±–ª–æ–∫,
    –≤ –∫–æ—Ç–æ—Ä–æ–º –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã (questions > 0).
    """
    text = text.strip()
    text = re.sub(r"```json", "", text)
    text = re.sub(r"```", "", text)

    results = []
    brace_stack = []
    start_idx = None

    for i, char in enumerate(text):
        if char == '{':
            if start_idx is None:
                start_idx = i
            brace_stack.append('{')
        elif char == '}':
            if brace_stack:
                brace_stack.pop()
                if not brace_stack and start_idx is not None:
                    results.append(text[start_idx:i+1].strip())
                    start_idx = None

    # –ü–∞—Ä—Å–∏–º —Å –∫–æ–Ω—Ü–∞ ‚Äî –∏—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞–ª–∏–¥–Ω—ã–π JSON —Å questions > 0
    for block in reversed(results):
        try:
            data = json.loads(block)

            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–π –∫–æ—Ä–Ω–µ–≤–æ–π "options", –µ—Å–ª–∏ —Å–ª—É—á–∞–π–Ω–æ –ø–æ–ø–∞–ª
            if isinstance(data, dict) and "options" in data and isinstance(data["options"], list):
                del data["options"]

            if "questions" in data and isinstance(data["questions"], list) and len(data["questions"]) > 0:
                return data
        except json.JSONDecodeError:
            continue

    return None  # –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ


def clean_json_keys(json_data):
    if isinstance(json_data, dict):
        return {k.strip(): clean_json_keys(v) for k, v in json_data.items()}
    elif isinstance(json_data, list):
        return [clean_json_keys(item) for item in json_data]
    return json_data


def process_text_with_mistral(text: str, prompt_type: str, filename: str = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"):
    try:
        with open(f"prompts/{prompt_type}_prompt.txt", encoding="utf-8") as f:
            prompt_template = f.read()
    except FileNotFoundError:
        raise ValueError(f"‚ùå –ü—Ä–æ–º—Ç prompts/{prompt_type}_prompt.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")

    safe_text = text.replace("{", "{{").replace("}", "}}")
    prompt = prompt_template.replace("{text}", safe_text)

    result = generator(
        prompt,
        max_new_tokens=1100,
        temperature=0.5,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=True
    )[0]["generated_text"]

    print("üîç –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n", result)
    extracted_json = extract_json(result)
    if not extracted_json:
        return {
            "type": prompt_type,
            "title": filename.rsplit(".", 1)[0],
            "questions": []
        }

    structured_data = extracted_json  # ‚ùó –£–±–∏—Ä–∞–µ–º json.loads

    structured_data = clean_json_keys(structured_data)
    structured_data.setdefault("type", prompt_type)

    # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Å—Ç—Ä–æ–∫—É
    title_val = structured_data.get("title")
    if not isinstance(title_val, str) or not title_val.strip():
        structured_data["title"] = filename.rsplit(".", 1)[0]

    structured_data.setdefault("questions", [])
    return structured_data
